<!DOCTYPE html>
<html>
<head>
    <style>
        h1 {text-align: center;}
        p {text-align: center;}
        div {text-align: center;}
        body { font-family: 'Courier New', Courier, monospace;}
        ul { list-style-position: inside; text-align: center;}
    </style>
</head>
<body>
<h1>Jason Brent Smith</h1>
    <p><a href="https://linkedin.com/in/jason-smith-009880142">Linkedin</a></p>
    <p><a href="https://scholar.google.com/citations?user=1R0IoRMAAAAJ">Publications</a></p>
    <p><a href="images/Jason Smith CV - Oct 2024.pdf">CV</a></p>
<div style="background-color: lightgrey;">
    <h2>About Me</h2>
    <h3> Research </h3>
        <p> I am a researcher interested in Human-AI Cocreativity. I have conducted research in a variety of areas related to AI-supported creativity: </p>
        <ul>
            <li> AI-based Interactive Music Systems </li>
            <li> Gesture Recognition </li>
            <li> Conversational Agents </li>
            <li> Accessible Technology </li>
            <li> Prosthetics for Musical Performance </li>
            <li> AI-based Scene Generation </li>
        </ul>
    <h3> Teaching </h3>
        <p> My goal in teaching and research mentorship is to provide students with skills they can use in industry and interdisciplinary research with a constructionist, project-based approach. </p>
        <ul>
        <li><b>Collaboration:</b> I encourage students to work together with others whose skills complement their own. </li>
        <li><b>Creativity:</b> I encourage students to work within the limitations of a course to develop projects they find useful to their research careers. </li>
        </ul>
    <h3> Biography </h3>
        <p>
            I am currently a Postdoctoral Scholar at Northwestern University in Evanston, Illinois.
            I work under Dr. Bryan Pardo at the Interactive Audio Lab in the Northwestern University Computer Science Department.
            I earned my BS in Music Engineering and Technology at the University of Miami in Miami, Florida in 2018.
            I earned my MS and Ph.D. in Music Technology at the Georgia Institute of Technology in Atlanta, Georgia in 2024.
        </p>
</div>
<div style="background-color: darkgray;">
    <h2>Projects</h2>
        <div style="background-color: darkgray;">
            <h3> Gestural Recognition & Human-AI Interaction </h3>
                <p> To explore how human musicians perceive the AI components of systems they use to perform, I developed multiple AI-based Interactive Music Systems as part of my dissertation titled "Human-AI Partnerships in Gesture-Controlled Interactive Music Systems." </p>
                <h4> Publications </h4>
                    <ul>
                        <li> Smith, Jason, and Freeman, Jason (2023). “Effects of Visual Explanation on Perceived Creative Autonomy in an AI-Based Generative Music System.” In IUI '23 Companion: Companion Proceedings of the 28th International Conference on Intelligent User Interfaces </li>
                        <li> Smith, Jason and Freeman, Jason (2022). “Human-AI Partnerships in Generative Music.” In the 21st International Conference on New Interfaces for Musical Expression </li>
                        <li> Smith, Jason, and Freeman, Jason (2021). "Effects of Deep Neural Networks on the Perceived Creative Autonomy of a Generative Musical System." In Proceedings of the 17th AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment </li>
                    </ul>
        </div>
        <div style="background-color: lightgray;">
            <h3> EarSketch </h3>
            <p> <a href="https://earsketch.gatech.edu/earsketch2/">EarSketch</a> is an online learning environment for coding and music education that I contributed to as a Graduate Research Assistant at Georgia Tech and Postdoctoral Scholar at Northwestern University. I contributed to two research projects: </p>
            <h4> EarSketch CAI </h4>
            Georgia Tech and the University of Florida collaborated on research and development of an experiemental conversational agent, a Co-Creative AI (CAI), to support EarSketch users in writing code and music.
            <h5> Publications </h5>
            <ul>
                <li> Rahimi, S., Smith, J.B., Truesdell, E.J.K., Vinay, A., Boyer, K.E., Magerko, B., Freeman, J., and McKlin, T. (2023). “Validity and Fairness of an Automated Assessment of Creativity in Computational Music Remixing.” Workshop on Automated Assessment and Guidance of Project Work at the 24th International Conference on Artificial Intelligence in Education</li>
                <li> Smith, J. B., Vinay, A., & Freeman, J. (2023). “The Impact of Salient Musical Features in a Hybrid Recommendation System for a Sound Library.” In the 3rd Workshop on Intelligent Music Interfaces for Listening and Creation (MILC) as part of the 28th International Conference on Intelligent User Interfaces </li>
                <li> Truesdell, Erin JK, et al. (2021). "Supporting Computational Music Remixing with a CoCreative Learning Companion." In Proceedings of the 2021 International Conference on Computational Creativity </li>
                <li> Smith, J., Truesdell, E., Freeman, J., Magerko, B., Boyer, K. E., & McKlin, T. (2020). “Modeling Music and Code Knowledge to Support a Co-Creative AI Agent for Education.” In Proceedings of the 21st International Society for Music Information Retrieval </li>
                <li> Smith, J., Jacob, M., Freeman, J., Magerko, B., & Mcklin, T. (2019). “Combining Collaborative and Content Filtering in a Recommendation System for a Web-based DAW.” In Proceedings of the 5th International Web Audio Conference </li>
                <li> Smith, J., Weeks, D., Jacob, M., Freeman, J., & Magerko, B. (2019). “Towards a Hybrid Recommendation System for a Sound Library.” In the 1st Workshop on Intelligent Music Interfaces for Listening and Creation (MILC) as part of the 28th International Conference on Intelligent User Interfaces </li>
            </ul>
            <h4> Accessible EarSketch </h4>
            Georgia Tech, Northwestern University, and the University of North Texas have collaborated on multi-stage research in making EarSketch more accessible for Blind and Visually Impaired (BVI) learners.
            <h5> Publications </h5>
            <ul>
                <li> Ding, S., Smith, J. B., Garrett, S., & Magerko, B. (2024). Redesigning EarSketch for Inclusive CS Education: A Participatory Design Approach. In Proceedings of the 23rd Annual ACM Interaction Design and Children Conference. </li>
                <li> Garrett, S., Smith, J. B., Blue, A., Ondin Z., Rempel, J., Mumma, K., Freeman, J., and Magerko, B. (2024). “Improving the Accessibility of the EarSketch Web-Based Audio Application for Blind and Visually Impaired Learners.” In Proceedings of the International Web Audio Conference </li>
            </ul>
        </div>
        <div style="background-color: darkgray;">
            <h3> Prosthetic Limbs for Musicians </h3>
                <p> As part of the Georgia Tech Robotic Musicicianship Group, I assisted in the development of two prosthetic limbs for musicians: </p>
                <h4> Skywalker: Five-Fingered Control </h4>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/UwsrzCVZAb8?si=ZutrA4cSb5ICu5h8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                <h4> Robotic Prosthetic Drumming Arm </h4>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/4O-rqn3BD7I?si=5fobJIgPbsgIhpiG" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div>
</div>
</body>
</html>